{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"from google.colab import drive\ndrive.mount('/content/drive')\n\n!unzip -qq /content/drive/MyDrive/RUPESH_RESEARCH_IMPLEMENTATIONS/DATASETS/NPY_DATA_patient_wise_split_128_x_128.zip","metadata":{"id":"5a9fCLmJQKsk","outputId":"d952e0a9-34bd-43d7-a2a8-80251b7876f5"}},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"/content/NPY_DATA_patient_wise_split\", ignore_errors = True)","metadata":{"id":"LGDg0wbDwQ9K","execution":{"iopub.status.busy":"2023-05-31T01:11:01.990182Z","iopub.execute_input":"2023-05-31T01:11:01.991414Z","iopub.status.idle":"2023-05-31T01:11:01.998937Z","shell.execute_reply.started":"2023-05-31T01:11:01.991344Z","shell.execute_reply":"2023-05-31T01:11:01.997354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install the required libraries","metadata":{}},{"cell_type":"code","source":"!pip install wandb --quiet\n!pip install torchsummary --quiet\n!pip install torchsampler --quiet\n!pip install torchmetrics --quiet\n!pip install grad-cam --quiet\n!pip install torchfunc==0.1.1 --quiet\n#!pip install timm --quiet\n#!pip install vit-pytorch --quiet\n!pip install -q timm pytorch-metric-learning","metadata":{"id":"fmYErKNS7AzA","outputId":"593bf5e4-9f7f-42b9-e331-27a8ca4bbc1e","execution":{"iopub.status.busy":"2023-05-31T01:11:02.001751Z","iopub.execute_input":"2023-05-31T01:11:02.002603Z","iopub.status.idle":"2023-05-31T01:12:25.745505Z","shell.execute_reply.started":"2023-05-31T01:11:02.002527Z","shell.execute_reply":"2023-05-31T01:12:25.744173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from glob import glob\n\n# os.makedirs(\"/content/PPMI_SPECT\", exist_ok= True)\n\n# zip_list = glob(f\"/content/drive/MyDrive/RUPESH_RESEARCH_IMPLEMENTATIONS/DATASETS/PPMI_SPECT_40_42/*\")\n# for fzip in zip_list:\n#     !unzip $fzip -d /content/PPMI_SPECT","metadata":{"id":"swb6n0m_8lhg","execution":{"iopub.status.busy":"2023-05-31T01:12:25.747549Z","iopub.execute_input":"2023-05-31T01:12:25.748240Z","iopub.status.idle":"2023-05-31T01:12:25.754028Z","shell.execute_reply.started":"2023-05-31T01:12:25.748187Z","shell.execute_reply":"2023-05-31T01:12:25.752583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torchfunc\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as tt\nfrom torchvision.ops import sigmoid_focal_loss\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nfrom torchsummary import summary\nfrom pytorch_metric_learning import losses\nfrom torch.cuda import amp\n\nimport random\n\nimport timm\n\nimport copy\n\nimport tarfile\n\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import f1_score\n\nfrom tqdm import tqdm\nimport wandb\n\nfrom glob import glob\n\nimport cv2 \nimport time\n\n\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'","metadata":{"id":"ZOwBZt6DNdSq","execution":{"iopub.status.busy":"2023-05-31T01:12:25.757567Z","iopub.execute_input":"2023-05-31T01:12:25.758300Z","iopub.status.idle":"2023-05-31T01:12:28.543668Z","shell.execute_reply.started":"2023-05-31T01:12:25.758261Z","shell.execute_reply":"2023-05-31T01:12:28.542420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timm.list_models(\"vit*\")","metadata":{"id":"3IlbRjHcO35f","outputId":"38994509-722c-439f-b8a3-ab080bad2769","execution":{"iopub.status.busy":"2023-05-31T01:12:28.545637Z","iopub.execute_input":"2023-05-31T01:12:28.546046Z","iopub.status.idle":"2023-05-31T01:12:28.557745Z","shell.execute_reply.started":"2023-05-31T01:12:28.546001Z","shell.execute_reply":"2023-05-31T01:12:28.556674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = timm.create_model(\"deit_tiny_patch16_224\", num_classes = 2, img_size = 128, pretrained = True)\n# #print(model)\n# device = get_default_device()\n# model =to_device(model, device)\n# x     = to_device(torch.randn(2, 3, 128, 128), device)\n# output = model(x)\n# print(output.shape)\n# summary(model, input_size = (3,128, 128))","metadata":{"id":"pV7u7f57O35h","execution":{"iopub.status.busy":"2023-05-31T01:12:28.559296Z","iopub.execute_input":"2023-05-31T01:12:28.561972Z","iopub.status.idle":"2023-05-31T01:12:28.567224Z","shell.execute_reply.started":"2023-05-31T01:12:28.561930Z","shell.execute_reply":"2023-05-31T01:12:28.566072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"id":"006JBA8b7Ay8","outputId":"72931559-07c0-4850-f0ed-25839efc8ab6","execution":{"iopub.status.busy":"2023-05-31T01:12:28.568874Z","iopub.execute_input":"2023-05-31T01:12:28.569627Z","iopub.status.idle":"2023-05-31T01:12:28.634525Z","shell.execute_reply.started":"2023-05-31T01:12:28.569549Z","shell.execute_reply":"2023-05-31T01:12:28.633304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.get_device_name()","metadata":{"id":"AiPvgmhPO35j","outputId":"91db3418-82d3-4989-ba12-ade1c155c888","execution":{"iopub.status.busy":"2023-05-31T01:12:28.636307Z","iopub.execute_input":"2023-05-31T01:12:28.636802Z","iopub.status.idle":"2023-05-31T01:12:28.656799Z","shell.execute_reply.started":"2023-05-31T01:12:28.636689Z","shell.execute_reply":"2023-05-31T01:12:28.655638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:12:28.658583Z","iopub.execute_input":"2023-05-31T01:12:28.658997Z","iopub.status.idle":"2023-05-31T01:12:28.666552Z","shell.execute_reply.started":"2023-05-31T01:12:28.658957Z","shell.execute_reply":"2023-05-31T01:12:28.665172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n!wandb login 4763b6e998a039526b08249362dd9c58a2348e34","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:12:28.672871Z","iopub.execute_input":"2023-05-31T01:12:28.673924Z","iopub.status.idle":"2023-05-31T01:12:32.698753Z","shell.execute_reply.started":"2023-05-31T01:12:28.673883Z","shell.execute_reply":"2023-05-31T01:12:32.697417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run from here","metadata":{}},{"cell_type":"code","source":"\n\n######################  CONFIGURATION  #########################\nconfig_dict= {\"num_epochs\": 300,\n              \n              # for LEARING RATE\n              \"lr\": 1e-5,\n              \"min_lr\": 1e-6,\n              \"opt_func\": \"AdamW\",\n              \"scheduler\": 'CosineAnnealingLR',\n              \"T_max\": 10,\n              \"weight_decay\": 1e-6,\n              \n              \"grad_clip\": 0.1,\n              \n              # for MODEL HYPERPARAMETERS\n              \"batch_size\": 16,\n              \"model\": \"resnet\",\n              \"num_classes\":2, \n              \"pretrained\": False,\n              \"random_seed\": 42,\n              \"target_names\": [\"HC\", \"SWEDD\"],\n              \"metric_task\": \"binary\",       \n              \n              # LOSS FUNCTION\n              \"Loss_fn\": \"focal\",                   # CE/ focal/ SCL\n                  # for FOCAL loss\n                  \"fl_alpha\": 0.25,                 # Must be in range [0, 1]\n                  \"fl_gamma\": 2,\n                  # for SCL\n                  \"temperature\": 0.1\n                }\n###############################################################\nproject_name = 'May_30_FL_PPMI_SPECT_128x128_2_class_HC_SWEDD_imbalance'\nproject_run_name = config_dict[\"model\"] + \"_pretrained_\" + str(config_dict[\"pretrained\"])\n###############################################################\n\nset_seed(seed = config_dict[\"random_seed\"])\n# torch.manual_seed(config_dict[\"random_seed\"])\n# if torch.cuda.is_available():\n#     torch.cuda.manual_seed_all(config_dict[\"random_seed\"])\n# torch.backends.cudnn.deterministic = True\n# np.random.seed(config_dict[\"random_seed\"])\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(config_dict['random_seed'])\n    #random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(config_dict[\"random_seed\"]) \n","metadata":{"id":"EXykMlKQO8wE","outputId":"c94d4237-e7a3-497f-d7cf-77ca8f9ceaa4","execution":{"iopub.status.busy":"2023-05-31T01:12:32.702175Z","iopub.execute_input":"2023-05-31T01:12:32.702796Z","iopub.status.idle":"2023-05-31T01:12:32.719244Z","shell.execute_reply.started":"2023-05-31T01:12:32.702745Z","shell.execute_reply":"2023-05-31T01:12:32.718065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str(config_dict[\"pretrained\"])","metadata":{"id":"k08khjMeQStK","outputId":"065305cf-fd8e-4184-9d89-ee392e10987e","execution":{"iopub.status.busy":"2023-05-31T01:12:32.720780Z","iopub.execute_input":"2023-05-31T01:12:32.721950Z","iopub.status.idle":"2023-05-31T01:12:32.735124Z","shell.execute_reply.started":"2023-05-31T01:12:32.721907Z","shell.execute_reply":"2023-05-31T01:12:32.733891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:12:32.737020Z","iopub.execute_input":"2023-05-31T01:12:32.737446Z","iopub.status.idle":"2023-05-31T01:12:32.744357Z","shell.execute_reply.started":"2023-05-31T01:12:32.737407Z","shell.execute_reply":"2023-05-31T01:12:32.743272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(config = config_dict,\n           name = project_run_name, \n           project=project_name,\n           notes=config_dict[\"model\"], \n           tags=['Adam', 'pretrained'])","metadata":{"id":"aUyqri2_78DZ","outputId":"5608bb78-dd8b-4ed9-c916-950ab6562961","execution":{"iopub.status.busy":"2023-05-31T01:12:32.746038Z","iopub.execute_input":"2023-05-31T01:12:32.746597Z","iopub.status.idle":"2023-05-31T01:13:07.875554Z","shell.execute_reply.started":"2023-05-31T01:12:32.746495Z","shell.execute_reply":"2023-05-31T01:13:07.874415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function","metadata":{"id":"KPuNwodQI6cy"}},{"cell_type":"code","source":"#WEIGHTED FOCAL LOSS\nclass WeightedFocalLoss(nn.Module):\n    \"Non weighted version of Focal Loss\"\n    def __init__(self, alpha=config_dict[\"fl_alpha\"], gamma=config_dict[\"fl_gamma\"]):\n        super(WeightedFocalLoss, self).__init__()\n        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n#         # targets = F.one_hot(targets, num_classes = 2)\n#         # print(inputs)\n#         # print(targets)\n        \n#         # print(type(inputs))\n#         # print(type(targets))\n\n#         # targets = targets.type(torch.long)\n#         # inputs = inputs.type(torch.long)\n#         #BCE_loss = nn.BCEWithLogitsLoss(inputs, targets, reduce = None)\n#         BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n        \n#         at = self.alpha.gather(0, targets.data.view(-1))\n#         pt = torch.exp(-BCE_loss)\n#         F_loss = at*(1-pt)**self.gamma * BCE_loss\n#         return F_loss.mean()\n        targets_one_hot = F.one_hot(targets, num_classes = config_dict['num_classes']).float()\n        loss = sigmoid_focal_loss(inputs, targets_one_hot, alpha = config_dict['fl_alpha'], gamma = config_dict['fl_gamma'], reduction = \"mean\")\n        return loss\n    \n# SUPERVISED CONTRASTIVE LOSS    \nclass SupervisedContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.1):\n        super(SupervisedContrastiveLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, feature_vectors, labels):\n        # Normalize feature vectors\n        feature_vectors_normalized = F.normalize(feature_vectors, p=2, dim=1)\n        # Compute logits\n        logits = torch.div(\n            torch.matmul(\n                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n            ),\n            self.temperature,\n        )\n        return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))","metadata":{"id":"b0LD6wF1HOiC","execution":{"iopub.status.busy":"2023-05-31T01:29:44.049755Z","iopub.execute_input":"2023-05-31T01:29:44.050147Z","iopub.status.idle":"2023-05-31T01:29:45.343851Z","shell.execute_reply.started":"2023-05-31T01:29:44.050111Z","shell.execute_reply":"2023-05-31T01:29:45.342691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input = torch.randn(3, requires_grad=True)\n# target = torch.empty(3).random_(2)\n# floss = WeightedFocalLoss()\n# loss = floss(input, target)\n# print(input, target)\n# loss.backward()\n# print(input)\n# print(input.shape, target.shape)\n","metadata":{"id":"NYMkbK3KOQdR","execution":{"iopub.status.busy":"2023-05-31T01:13:09.229341Z","iopub.execute_input":"2023-05-31T01:13:09.231967Z","iopub.status.idle":"2023-05-31T01:13:10.187034Z","shell.execute_reply.started":"2023-05-31T01:13:09.231923Z","shell.execute_reply":"2023-05-31T01:13:10.185978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET reading from the disk","metadata":{"id":"8llJkBgEI8T5"}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, imgs_path, tfms):\n        self.imgs_path = imgs_path\n        class_list = os.listdir(imgs_path)\n        print(class_list)\n        class_list.sort()\n        self.data = []\n        for class_path in class_list:\n            class_name = class_path\n            # print(glob(os.path.join(imgs_path, class_path,\"*.npz\")))\n            for img_path in glob(os.path.join(imgs_path, class_path,\"*.npy\")):\n                self.data.append([img_path, class_name])\n        #print(self.data)\n        class_map = {}\n        for i in range(len(class_list)):\n            class_map[class_list[i]] = i\n        self.class_map = class_map\n        print(class_map)\n        for key, val in self.class_map.items():\n            print(f\"{key}: {val}\")\n        self.img_dim = (128, 128)  \n        self.tfms = tfms  \n\n    def __len__(self):\n        return len(self.data)  \n\n    def __class_to_idx__(self):\n        for key, val in self.class_map.items():\n            print(f\"{key}: {val}\")\n    \n    def __getitem__(self, idx):\n        img_path, class_name = self.data[idx]\n        img = np.load(img_path)\n        img = (img - img.min())/(img.max() - img.min())\n        class_id = self.class_map[class_name]\n        # print(img.max(), img.min(), img.dtype)\n        img_tensor = torch.from_numpy(img).type(torch.float32)\n        # print(img_tensor.max() , img_tensor.min(), img_tensor.dtype)\n        img_tensor = self.tfms(img_tensor)\n        #print(img_tensor.max() , img_tensor.min(), img_tensor.dtype)\n        class_id = torch.tensor(class_id)\n        return img_tensor, class_id\n\nclass CustomDataset_patient_wise(Dataset):\n    def __init__(self, imgs_path, tfms):\n        self.imgs_path = imgs_path\n        class_list = os.listdir(imgs_path)\n        class_list.sort()\n        print(class_list)\n        self.data = []\n        for class_path in class_list:\n            class_name = class_path\n            print((os.path.join(imgs_path, class_path,\"**/*.npy\")))\n            for img_path in glob(os.path.join(imgs_path, class_path,\"**/*.npy\")):\n                self.data.append([img_path, class_name])\n        #print(self.data)\n        self.OVERALL_MAX, self.OVERALL_MIN = -500000.0, 500000.0\n        for fpath in tqdm(self.data, desc = \"Processing the data for finding local MIN and MAX .....\"):\n            #print(fpath)\n            x = np.load(fpath[0])\n            self.OVERALL_MAX = max(x.max(), self.OVERALL_MAX)\n            self.OVERALL_MIN = min(x.min(), self.OVERALL_MIN)\n        print(f\"DATASET_MAX: {self.OVERALL_MAX}, DATASET_MIN: {self.OVERALL_MIN}\")\n        class_map = {}\n        for i in range(len(class_list)):\n            class_map[class_list[i]] = i\n        self.class_map = class_map\n        print(class_map)\n        for key, val in self.class_map.items():\n            print(f\"{key}: {val}\")\n        self.img_dim = x.shape\n        print(self.img_dim)\n        self.tfms = tfms  \n\n    def __len__(self):\n        return len(self.data)  \n\n    def __class_to_idx__(self):\n        for key, val in self.class_map.items():\n            print(f\"{key}: {val}\")\n    \n    def __getitem__(self, idx):\n        img_path, class_name = self.data[idx]\n        img = np.load(img_path)\n        img = (img - self.OVERALL_MIN)/(self.OVERALL_MAX - self.OVERALL_MIN)\n        class_id = self.class_map[class_name]\n        # print(img.max(), img.min(), img.dtype)\n        img_tensor = torch.from_numpy(img).type(torch.float32)\n        # print(img_tensor.max() , img_tensor.min(), img_tensor.dtype)\n        if self.tfms is not None:\n            img_tensor = self.tfms(img_tensor)\n        #print(img_tensor.max() , img_tensor.min(), img_tensor.dtype)\n        class_id = torch.tensor(class_id)\n        return img_tensor, class_id","metadata":{"id":"ssdC1e3WO35o","execution":{"iopub.status.busy":"2023-05-31T01:13:10.192215Z","iopub.execute_input":"2023-05-31T01:13:10.194742Z","iopub.status.idle":"2023-05-31T01:13:11.418407Z","shell.execute_reply.started":"2023-05-31T01:13:10.194700Z","shell.execute_reply":"2023-05-31T01:13:11.417337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_ds.__class_to_idx__()","metadata":{"id":"uTlwVcgLO35p","execution":{"iopub.status.busy":"2023-05-31T01:13:11.423830Z","iopub.execute_input":"2023-05-31T01:13:11.426419Z","iopub.status.idle":"2023-05-31T01:13:12.502375Z","shell.execute_reply.started":"2023-05-31T01:13:11.426375Z","shell.execute_reply":"2023-05-31T01:13:12.501322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformations and Augmentations","metadata":{}},{"cell_type":"code","source":"stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\ntfms_org = tt.Compose([#tt.ToTensor(),\n                   # tt.Resize((224, 224)),\n                   tt.Lambda(lambda x: x.repeat(int(3), 1, 1)),\n                   tt.Normalize(*stats,inplace=True)])\n\ntfms_1 = tt.Compose([#tt.ToTensor(),\n                   # tt.Resize((224, 224)),\n                   tt.Lambda(lambda x: x.repeat(int(3), 1, 1)),\n                   tt.RandomRotation(10),\n                   \n                   tt.Normalize(*stats,inplace=True)])\n\ntfms_2 = tt.Compose([#tt.ToTensor(),\n                   # tt.Resize((224, 224)),\n                   tt.Lambda(lambda x: x.repeat(int(3), 1, 1)),\n                   tt.RandomHorizontalFlip(0.5),\n                   \n                   tt.Normalize(*stats,inplace=True)])\n\ntfms_3 = tt.Compose([#tt.ToTensor(),\n                   # tt.Resize((224, 224)),\n                   tt.Lambda(lambda x: x.repeat(int(3), 1, 1)),\n                   tt.GaussianBlur(3, sigma = (0.1, 2.0)),\n                   \n                   tt.Normalize(*stats,inplace=True)])","metadata":{"id":"n3L9Ius0Xjiz","execution":{"iopub.status.busy":"2023-05-31T01:13:12.507602Z","iopub.execute_input":"2023-05-31T01:13:12.510193Z","iopub.status.idle":"2023-05-31T01:13:13.684366Z","shell.execute_reply.started":"2023-05-31T01:13:12.510149Z","shell.execute_reply":"2023-05-31T01:13:13.683252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train, Valid and Test Dataset read","metadata":{}},{"cell_type":"code","source":"# PyTorch datasets\n\n\"\"\"NOTE in this version the tfms are ignored\"\"\"\ndata_dir = \"/kaggle/input/npy-train-test-split-may-30/npy_train_test_split\"\n\ntrain_ds_org = CustomDataset_patient_wise(data_dir+'/train', tfms_org)\n# train_ds_1 = CustomDataset_patient_wise(data_dir+'/train', tfms_1)\n# train_ds_2 = CustomDataset_patient_wise(data_dir+'/train', tfms_2)\n# train_ds_3 = CustomDataset_patient_wise(data_dir+'/train', tfms_3)\n\ntrain_ds = train_ds_org #+ train_ds_1 + train_ds_2 + train_ds_3           #Augmentation is ONLY for the Training dataset, test dataset must have original DATA\nvalid_ds = CustomDataset_patient_wise(data_dir+'/valid', tfms = tfms_org)\ntest_ds  = CustomDataset_patient_wise(data_dir+'/test', tfms = tfms_org)","metadata":{"id":"nsIkzejcYBhf","outputId":"57a8b03f-5ab3-457c-9e12-6025a4182403","execution":{"iopub.status.busy":"2023-05-31T01:13:13.689594Z","iopub.execute_input":"2023-05-31T01:13:13.692189Z","iopub.status.idle":"2023-05-31T01:13:31.432448Z","shell.execute_reply.started":"2023-05-31T01:13:13.692142Z","shell.execute_reply":"2023-05-31T01:13:31.431044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds.__len__()","metadata":{"id":"cIzEuPdDc4Kx","outputId":"fd558550-e473-4c6a-f969-dc70cd379864","execution":{"iopub.status.busy":"2023-05-31T01:13:31.434344Z","iopub.execute_input":"2023-05-31T01:13:31.435091Z","iopub.status.idle":"2023-05-31T01:13:32.460769Z","shell.execute_reply.started":"2023-05-31T01:13:31.435048Z","shell.execute_reply":"2023-05-31T01:13:32.459432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoaders","metadata":{"id":"N74vcFsaJBGk"}},{"cell_type":"code","source":"batch_size = config_dict[\"batch_size\"]\n# PyTorch data loaders\ntrain_dl = DataLoader(train_ds, batch_size, num_workers=2, pin_memory=True, shuffle = True, worker_init_fn=seed_worker,\n    generator=g)\nvalid_dl = DataLoader(valid_ds, batch_size, num_workers=2, pin_memory=True, shuffle = True, worker_init_fn=seed_worker,\n    generator=g)\ntest_dl = DataLoader(test_ds, batch_size, num_workers =2, pin_memory = True, shuffle = True, worker_init_fn=seed_worker,\n    generator=g)","metadata":{"id":"YkOj4YBEKbIw","execution":{"iopub.status.busy":"2023-05-31T01:13:32.462603Z","iopub.execute_input":"2023-05-31T01:13:32.463382Z","iopub.status.idle":"2023-05-31T01:13:33.541945Z","shell.execute_reply.started":"2023-05-31T01:13:32.463341Z","shell.execute_reply":"2023-05-31T01:13:33.540701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temp = train_dl.__iter__()\n# print(temp.size)","metadata":{"id":"VgmnnC32-gaS","execution":{"iopub.status.busy":"2023-05-31T01:13:33.543981Z","iopub.execute_input":"2023-05-31T01:13:33.544727Z","iopub.status.idle":"2023-05-31T01:13:34.767665Z","shell.execute_reply.started":"2023-05-31T01:13:33.544682Z","shell.execute_reply":"2023-05-31T01:13:34.766460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl.__len__()","metadata":{"id":"u_QWraBnASgJ","outputId":"a556be70-c50c-4b3c-abf1-9499b9c0baec","execution":{"iopub.status.busy":"2023-05-31T01:13:34.769113Z","iopub.execute_input":"2023-05-31T01:13:34.770228Z","iopub.status.idle":"2023-05-31T01:13:35.829725Z","shell.execute_reply.started":"2023-05-31T01:13:34.770184Z","shell.execute_reply":"2023-05-31T01:13:35.828428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Denormalize images and view a batch of images","metadata":{"id":"xz607O3zJEc0"}},{"cell_type":"code","source":"def denormalize(images, means, stds):\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        denorm_images = denormalize(images, *stats).numpy()\n        print((denorm_images.max()))\n        for imgNum in range(len(denorm_images)):\n            denorm_images[imgNum, :, :, :] = cv2.normalize(denorm_images[imgNum, :, :, :], None, 0, 255, cv2.NORM_MINMAX)\n        denorm_images = torch.from_numpy(denorm_images) \n        ax.imshow(make_grid(denorm_images[:16], nrow=4).permute(1, 2, 0).clamp(0,1))\n        break","metadata":{"id":"iBECX6A9ILcU","execution":{"iopub.status.busy":"2023-05-31T01:13:35.835098Z","iopub.execute_input":"2023-05-31T01:13:35.837441Z","iopub.status.idle":"2023-05-31T01:13:36.919782Z","shell.execute_reply.started":"2023-05-31T01:13:35.837405Z","shell.execute_reply":"2023-05-31T01:13:36.918528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl)","metadata":{"id":"WX8aCiXNIMfh","execution":{"iopub.status.busy":"2023-05-31T01:13:36.921602Z","iopub.execute_input":"2023-05-31T01:13:36.922394Z","iopub.status.idle":"2023-05-31T01:13:41.297090Z","shell.execute_reply.started":"2023-05-31T01:13:36.922350Z","shell.execute_reply":"2023-05-31T01:13:41.295824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility functions\n - choosing from available devices\n - Loading the data/ model into the DEFAULT device","metadata":{"id":"ufaKC2N9JJkX"}},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"id":"IkFiRFptdtS2","execution":{"iopub.status.busy":"2023-05-31T01:13:41.298858Z","iopub.execute_input":"2023-05-31T01:13:41.299550Z","iopub.status.idle":"2023-05-31T01:13:42.379979Z","shell.execute_reply.started":"2023-05-31T01:13:41.299502Z","shell.execute_reply":"2023-05-31T01:13:42.378730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"id":"L-uUWDcqet-f","outputId":"433f7000-294f-4684-b434-cce2f4252054","execution":{"iopub.status.busy":"2023-05-31T01:13:42.391649Z","iopub.execute_input":"2023-05-31T01:13:42.392233Z","iopub.status.idle":"2023-05-31T01:13:43.160636Z","shell.execute_reply.started":"2023-05-31T01:13:42.392199Z","shell.execute_reply":"2023-05-31T01:13:43.159335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)","metadata":{"id":"FDU7pzydevt9","execution":{"iopub.status.busy":"2023-05-31T01:13:43.166572Z","iopub.execute_input":"2023-05-31T01:13:43.166949Z","iopub.status.idle":"2023-05-31T01:13:44.255909Z","shell.execute_reply.started":"2023-05-31T01:13:43.166916Z","shell.execute_reply":"2023-05-31T01:13:44.254675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Codes","metadata":{"id":"CCgP7RMOMIe0"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport torchmetrics\nfrom torchmetrics import AUROC\n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\ndef training_step(model, batch, criterion):\n    images, labels = batch \n    out = model(images)                  # Generate predictions\n    loss = criterion(out, labels)\n#     if(config_dict[\"Loss_fn\"] == \"CE\"):\n#         loss = F.cross_entropy(out, labels) # Calculate loss\n#     elif(config_dict[\"Loss_fn\"] == \"focal\"):\n#         focal_loss = WeightedFocalLoss()\n#         #print(out)\n#         loss = focal_loss(out, labels)\n    return loss\n\n@torch.no_grad()\ndef validation_step(model, batch, criterion, fold):\n    images, labels = batch \n    out = model(images)                    # Generate predictions\n    pred = out\n    #print(pred)\n    loss = criterion(out, labels)\n#     if(config_dict[\"Loss_fn\"] == \"CE\"):\n#         loss = F.cross_entropy(out, labels) # Calculate loss\n#     elif(config_dict[\"Loss_fn\"] == \"focal\"):\n#         focal_loss = WeightedFocalLoss()\n#         loss = focal_loss(out, labels)\n#     elif(config_dict[\"Loss_fn\"] == \"SCL\"):\n#         focal_loss = \n    acc = accuracy(out, labels)           # Calculate accuracy\n    return {f'val_loss_{fold}': loss.detach(), f'val_acc_{fold}': acc, f\"preds_{fold}\": pred, f\"labels_{fold}\": labels}\n\n@torch.no_grad()\ndef validation_epoch_end(outputs, fold):\n    batch_losses = [x[f'val_loss_{fold}'] for x in outputs]\n    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n    batch_accs = [x[f'val_acc_{fold}'] for x in outputs]\n    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n    batch_y_true = [x[f\"labels_{fold}\"].detach() for x in outputs]\n    batch_y_pred = [x[f\"preds_{fold}\"].detach() for x in outputs]\n    y_true = torch.cat(batch_y_true)\n    y_pred = torch.cat(batch_y_pred)\n    f1 = torchmetrics.functional.f1_score(y_pred, y_true, num_classes=config_dict[\"num_classes\"],task = 'multiclass', average = \"micro\")\n    auroc = AUROC(num_classes=config_dict[\"num_classes\"], average=\"macro\", task = 'multiclass')\n    auroc = auroc(y_pred, y_true)\n    return {f'val_loss_{fold}': epoch_loss.item(), f'val_acc_{fold}': epoch_acc.item(), f\"val_f1_{fold}\": f1, f\"val_auroc_{fold}\": auroc}\n\ndef epoch_end(epoch, result, fold):\n    print(\"Epoch [{}], \\ntrain_acc: {:.4f}, train_loss: {:.4f}, train_f1: {:.4f}, train_auroc: {:4f}, \\nval_acc: {:.4f}, val_loss: {:.4f}, val_f1: {:.4f}, val_auroc: {:.4f}\".format(\n        epoch, result[f'train_acc_{fold}'], result[f'train_loss_{fold}'], result[f'train_f1_{fold}'], result[f\"train_auroc_{fold}\"], result[f'val_acc_{fold}'], result[f'val_loss_{fold}'], result[f'val_f1_{fold}'], result[f\"val_auroc_{fold}\"]))\n\n    \n","metadata":{"id":"F30f6Poya67q","execution":{"iopub.status.busy":"2023-05-31T01:13:44.257720Z","iopub.execute_input":"2023-05-31T01:13:44.258341Z","iopub.status.idle":"2023-05-31T01:13:53.740971Z","shell.execute_reply.started":"2023-05-31T01:13:44.258301Z","shell.execute_reply":"2023-05-31T01:13:53.739316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definitions","metadata":{"id":"Yh3SU6weMK-r"}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\n\n\"\"\"AlexNet model\"\"\"\nif(config_dict['model'] == \"alexnet\"):\n    print(\"alexnet\")\n    if (config_dict[\"pretrained\"]==True):\n        model = models.alexnet(pretrained=config_dict[\"pretrained\"])\n        model.classifier._modules[\"6\"] = nn.Linear(model.classifier._modules[\"6\"].in_features, config_dict[\"num_classes\"])\n        #model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    else:\n        model = models.alexnet(pretrained=config_dict[\"pretrained\"], num_classes=config_dict[\"num_classes\"])\n        #model.features[0] = nn.Conv2d(1, 64, kernel_size=(nn.BCEWithLogitsLoss11, 11), stride=(4, 4), padding=(2, 2))\n    print(model)\n#model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n\n\n\"\"\"VGG16\"\"\"\nif(config_dict['model'] == \"vgg16\"):\n    if (config_dict[\"pretrained\"]==True):\n        model = models.vgg16(pretrained=config_dict[\"pretrained\"])\n        model.classifier._modules[\"6\"] = nn.Linear(model.classifier._modules[\"6\"].in_features, config_dict[\"num_classes\"])\n    else:\n        model = models.vgg16(pretrained=config_dict[\"pretrained\"], num_classes=config_dict[\"num_classes\"])\n    print(model)\n# model.features[0] = nn.Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n\"\"\"VGG19\"\"\"\nif(config_dict['model'] == \"vgg19\"):\n    if (config_dict[\"pretrained\"]==True):\n        model = models.vgg19(pretrained=config_dict[\"pretrained\"])\n        model.classifier._modules[\"6\"] = nn.Linear(model.classifier._modules[\"6\"].in_features, config_dict[\"num_classes\"])\n    else:\n        model = models.vgg19(pretrained=config_dict[\"pretrained\"], num_classes=config_dict[\"num_classes\"])\n    print(model)\n#model = models.vgg19(pretrained=False, num_classes=2)\n\n\"\"\"resnet\"\"\"\nif(config_dict['model'] == \"resnet\"):\n    if (config_dict[\"pretrained\"]==True):\n        model = timm.create_model(\"resnet18\", pretrained=config_dict[\"pretrained\"], num_classes = config_dict[\"num_classes\"])\n        #model = models.resnet18(pretrained=config_dict[\"pretrained\"])\n        #model.fc = nn.Linear(model.fc.in_features, config_dict[\"num_classes\"])\n    else:\n        #model = models.resnet18(pretrained=config_dict[\"pretrained\"], num_classes = config_dict[\"num_classes\"])\n        model = timm.create_model(\"resnet18\", pretrained=config_dict[\"pretrained\"], num_classes = config_dict[\"num_classes\"])\n    print(model)\n    \n\"\"\"EfficientNET - V2\"\"\"\nif(config_dict['model'] == \"effnetv2\"):\n    if (config_dict[\"pretrained\"]==True):\n        model = timm.create_model(\"efficientnetv2_rw_t\", pretrained=config_dict[\"pretrained\"], num_classes = config_dict[\"num_classes\"])\n        #model.fc = nn.Linear(model.fc.in_features, config_dict[\"num_classes\"])\n    else:\n        model = timm.create_model(\"efficientnetv2_rw_t\", pretrained=config_dict[\"pretrained\"], num_classes = config_dict[\"num_classes\"])\n    print(model) \n\n\"\"\"DeiT\"\"\"\nif(config_dict['model'] == \"DeiT\"):\n    if (config_dict[\"pretrained\"]==True):\n        model = timm.create_model(\"deit_tiny_patch16_224\", num_classes = config_dict[\"num_classes\"], img_size = 128, pretrained = config_dict[\"pretrained\"])\n        #model.fc = nn.Linear(model.fc.in_features, config_dict[\"num_classes\"])\n    else:\n        model = timm.create_model(\"deit_tiny_patch16_224\", num_classes = config_dict[\"num_classes\"], img_size = 128, pretrained = config_dict[\"pretrained\"])\n    print(model) \n\n\"\"\"ViT\"\"\"\nif(config_dict['model'] == \"ViT\"):\n    if (config_dict[\"pretrained\"]==True):\n        model = timm.create_model(\"vit_tiny_patch16_224\", num_classes = config_dict[\"num_classes\"], img_size = 128, pretrained = config_dict[\"pretrained\"])\n        #model.fc = nn.Linear(model.fc.in_features, config_dict[\"num_classes\"])\n    else:\n        model = timm.create_model(\"vit_tiny_patch16_224\", num_classes = config_dict[\"num_classes\"], img_size = 128, pretrained = config_dict[\"pretrained\"])\n    print(model) \n\n# \"\"\"Vision Transformer\"\"\"\n# from torchvision.models import resnet50\n\n# from vit_pytorch.distill import DistillableViT, DistillWrapper\n# from vit_pytorch import ViT, SimpleViT\n# from vit_pytorch.cct import CCT\n\n# if(config_dict['model'] == \"ViT\"):\n#     # if(config_dict[\"pretrained\"]==True):\n#     #     model = \n        \n#     # else:\n#     model = ViT(\n#                 image_size = 128,\n#                 patch_size = 8,\n#                 num_classes = 2,\n#                 dim = 1024,\n#                 depth = 6,\n#                 heads = 8,\n#                 mlp_dim = 2048,\n#                 dropout = 0.1,\n#                 emb_dropout = 0.1\n#                 )\n#     print(model)     \n# print(model.fc)\n# for param in model.parameters():\n#     param.requires_grad = False\n    # Replace the last fully-connected layer\n    # Parameters of newly constructed modules have requires_grad=True by default\n#model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n#print(model)\n\n\n\n# model.training_step = training_step\n# model.validation_step = validation_step\n# model.validation_epoch_end = validation_epoch_end\n# model.epoch_end = epoch_end\n\n#print(model)\n\n\n    \nmodel = to_device(model, device)\n\nwandb.watch(model)\n\n\nsummary(model, input_size = (3, 128, 128))\n","metadata":{"id":"Km0xMa3Qe9jF","outputId":"251da832-8619-418f-fa59-8196928327da","execution":{"iopub.status.busy":"2023-05-31T01:13:53.744305Z","iopub.execute_input":"2023-05-31T01:13:53.744739Z","iopub.status.idle":"2023-05-31T01:14:01.056575Z","shell.execute_reply.started":"2023-05-31T01:13:53.744695Z","shell.execute_reply":"2023-05-31T01:14:01.055307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.EfficientNet.conv_head","metadata":{"id":"2swJXs-YO35w","execution":{"iopub.status.busy":"2023-05-31T01:14:01.058369Z","iopub.execute_input":"2023-05-31T01:14:01.059023Z","iopub.status.idle":"2023-05-31T01:14:02.236920Z","shell.execute_reply.started":"2023-05-31T01:14:01.058982Z","shell.execute_reply":"2023-05-31T01:14:02.235636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader, fold):\n    model.eval()\n    outputs = [validation_step(model, batch, criterion, fold) for batch in val_loader]\n    return validation_epoch_end(outputs, fold)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_fold(epochs, lr, model, train_loader, val_loader, test_dl, grad_clip = config_dict[\"grad_clip\"],\n                 weight_decay = config_dict[\"weight_decay\"], opt_func=torch.optim.SGD, criterion = F.cross_entropy,\n                fold = 0):\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n    best_acc = -np.inf\n    history = []\n    optimizer = opt_func(model.parameters(), lr = lr, weight_decay = config_dict[\"weight_decay\"])\n    sched = lr_scheduler.CosineAnnealingLR(optimizer, T_max=config_dict[\"T_max\"], eta_min=config_dict[\"min_lr\"])\n    lrs = []\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = training_step(model, batch, criterion)\n            train_losses.append(loss)\n            loss.backward()\n            \n            ## GRADIENT CLIPPING\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n                 \n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        with torch.no_grad():\n            # Validation phase\n            result = evaluate(model, val_loader, fold)\n            if(result[f'val_loss_{fold}']<=best_loss):\n                    best_loss = result[f'val_loss_{fold}']\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                    PATH = f\"Fold{fold}_{best_loss:.2f}_epoch_{epoch}.bin\"\n#                     torch.save(model.state_dict(), PATH)\n\n            result_2 = evaluate(model, train_loader, fold)\n            result[f'train_acc_{fold}'] = result_2[f'val_acc_{fold}']\n            result[f'train_loss_{fold}'] = torch.stack(train_losses).mean().item()\n            result[f'train_f1_{fold}'] = result_2[f\"val_f1_{fold}\"]\n            result[f\"train_auroc_{fold}\"] = result_2[f\"val_auroc_{fold}\"]\n            result[f\"epoch_{fold}\"] = epoch\n            wandb.log({'Epoch': epoch,\n                       \"fold\": fold,\n                      \"train_acc\": result[f'train_acc_{fold}'],\n                      \"train_loss\": result[f'train_loss_{fold}'],\n                      \"train_f1\": result[f'train_f1_{fold}'],\n                      \"val_acc\": result[f'val_acc_{fold}'],\n                      \"val_loss\": result[f'val_loss_{fold}'],\n                      \"val_f1\": result[f'val_f1_{fold}']})\n\n            epoch_end(epoch, result, fold)\n            history.append(result)\n    # Testing phase\n    result_temp = evaluate(model, test_dl, fold)\n    test_result = {\"test_acc\": result_temp[f'val_acc_{fold}'],\n                  \"test_loss\": result_temp[f'val_loss_{fold}'],\n                  \"test_f1\": result_temp[f'val_f1_{fold}']}\n    wandb.log(test_result)\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss \",best_loss)\n    print(\"Best acc \", best_acc)\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","metadata":{"id":"5NY5vjiZe-c7","execution":{"iopub.status.busy":"2023-05-31T01:14:02.238658Z","iopub.execute_input":"2023-05-31T01:14:02.239289Z","iopub.status.idle":"2023-05-31T01:14:03.274371Z","shell.execute_reply.started":"2023-05-31T01:14:02.239248Z","shell.execute_reply":"2023-05-31T01:14:03.272959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = config_dict[\"num_epochs\"]\nlr = config_dict[\"lr\"]\n\n# for fixing OPTIMIZER\nif config_dict[\"opt_func\"] == \"AdamW\":\n    opt_func = torch.optim.AdamW \nelif config_dict[\"opt_func\"] == \"Adam\":\n    opt_func = torch.optim.Adam\nelif config_dict[\"opt_func\"] == \"RMSprop\":\n    opt_func = torch.optim.RMSprop\n\ndef cross_entropy(inputs, targets):\n    pass\n\n# for fixing LOSS\nif config_dict[\"Loss_fn\"] == \"CE\":\n    criterion = F.cross_entropy().to(device)\nelif config_dict[\"Loss_fn\"] == 'focal':\n    criterion = WeightedFocalLoss().to(device)\nelif config_dict[\"Loss_fn\"] == 'SCL':\n    criterion = SupervisedContrastiveLoss(temperature=config_dict[\"temperature\"]).to(device)\n\nprint(opt_func)","metadata":{"id":"2TR1WwdrfAVX","outputId":"16bd7205-5bff-4df5-b8fe-f7b5d440a863","execution":{"iopub.status.busy":"2023-05-31T01:29:49.492491Z","iopub.execute_input":"2023-05-31T01:29:49.493251Z","iopub.status.idle":"2023-05-31T01:29:50.641273Z","shell.execute_reply.started":"2023-05-31T01:29:49.493209Z","shell.execute_reply":"2023-05-31T01:29:50.640183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom torch.optim import lr_scheduler\n\nhistory = []\nmodel, history_temp = fit_one_fold(epochs, lr, model, train_dl, valid_dl, test_dl, opt_func=opt_func, criterion = criterion, fold = 0)\nhistory += history_temp","metadata":{"id":"tQDwlHcMzRAZ","outputId":"9d62897b-0caa-4c45-9941-269ec225cb2f","execution":{"iopub.status.busy":"2023-05-31T01:29:50.643829Z","iopub.execute_input":"2023-05-31T01:29:50.644604Z","iopub.status.idle":"2023-05-31T01:36:37.898239Z","shell.execute_reply.started":"2023-05-31T01:29:50.644563Z","shell.execute_reply":"2023-05-31T01:36:37.897041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = F.one_hot(torch.tensor([1,0,1,0,1,1,1,1]), num_classes = 2) \nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:36:37.900307Z","iopub.execute_input":"2023-05-31T01:36:37.900953Z","iopub.status.idle":"2023-05-31T01:36:39.201791Z","shell.execute_reply.started":"2023-05-31T01:36:37.900904Z","shell.execute_reply":"2023-05-31T01:36:39.200551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_result = evaluate(model, test_dl, fold = 0)\nprint(test_result)","metadata":{"id":"uvL1EUwg7AzU","outputId":"c09bc883-fee6-40f1-f1e6-6fe48af15e1d","execution":{"iopub.status.busy":"2023-05-31T01:36:48.768393Z","iopub.execute_input":"2023-05-31T01:36:48.769158Z","iopub.status.idle":"2023-05-31T01:36:50.639168Z","shell.execute_reply.started":"2023-05-31T01:36:48.769115Z","shell.execute_reply":"2023-05-31T01:36:50.631700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((model))","metadata":{"id":"6IIjYmhcuVPS","outputId":"986966e8-88af-4dcd-bad9-478d1406dbe0","execution":{"iopub.status.busy":"2023-05-31T01:36:56.504902Z","iopub.execute_input":"2023-05-31T01:36:56.505476Z","iopub.status.idle":"2023-05-31T01:36:57.867231Z","shell.execute_reply.started":"2023-05-31T01:36:56.505431Z","shell.execute_reply":"2023-05-31T01:36:57.866161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion matrix","metadata":{"id":"UPZ1eQSFiimN"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd\n\ny_pred = []\ny_true = []\n\n# iterate over test data\nfor inputs, labels in test_dl:\n        output = model(inputs) # Feed Network\n\n        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n        y_pred.extend(output) # Save Prediction\n        \n        labels = labels.data.cpu().numpy()\n        y_true.extend(labels) # Save Truth\n\n# constant for classes\nclasses = tuple(config_dict[\"target_names\"])\ntarget_names = config_dict[\"target_names\"]\n\n# Build confusion matrix\ncf_matrix = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(cf_matrix, index = [i for i in classes],\n                     columns = [i for i in classes])\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 22}\n\nmatplotlib.rc('font', **font)\nplt.figure(figsize = (12,7))\nsn.heatmap(df_cm, annot=True, fmt = \"g\")\n\"\"\"For Google-Colab\"\"\"\n#BASE_PATH = os.getcwd()\n\"\"\"For Kaggle\"\"\"\nBASE_PATH = \"/kaggle/working\"\nmodel_root_folder = os.path.join(BASE_PATH, project_name, config_dict[\"model\"])\nos.makedirs(model_root_folder, exist_ok = True)\ncm_figName = os.path.join(model_root_folder, config_dict[\"model\"] + \"_Confusion_Matrix.png\")\nplt.savefig(cm_figName)\n\nimport sklearn\nprint(sklearn.metrics.classification_report(y_true, y_pred,target_names = target_names))","metadata":{"id":"ivxMVSlJSO9G","outputId":"587d579f-bcdb-4b37-c041-64b70f1fb462","execution":{"iopub.status.busy":"2023-05-31T01:37:13.422509Z","iopub.execute_input":"2023-05-31T01:37:13.422909Z","iopub.status.idle":"2023-05-31T01:37:15.886923Z","shell.execute_reply.started":"2023-05-31T01:37:13.422872Z","shell.execute_reply":"2023-05-31T01:37:15.885709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.save(\"/content/\"+cm_figName, base_path = model_root_folder)","metadata":{"id":"ZQQSYEL9pFgR","execution":{"iopub.status.busy":"2023-05-31T01:37:15.889702Z","iopub.execute_input":"2023-05-31T01:37:15.890392Z","iopub.status.idle":"2023-05-31T01:37:17.171544Z","shell.execute_reply.started":"2023-05-31T01:37:15.890346Z","shell.execute_reply":"2023-05-31T01:37:17.170326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# classification report","metadata":{"id":"yCnvBpURifwB"}},{"cell_type":"code","source":"report = sklearn.metrics.classification_report(y_true, y_pred,target_names = target_names, output_dict = True)\nprint(report)","metadata":{"id":"HXZY16ZwqT6k","outputId":"f561acb9-1226-4a12-8be6-0ead764f933f","execution":{"iopub.status.busy":"2023-05-31T01:37:17.173319Z","iopub.execute_input":"2023-05-31T01:37:17.174034Z","iopub.status.idle":"2023-05-31T01:37:18.364823Z","shell.execute_reply.started":"2023-05-31T01:37:17.173991Z","shell.execute_reply":"2023-05-31T01:37:18.363557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(report).transpose()","metadata":{"id":"aCQE-b3xeAid","execution":{"iopub.status.busy":"2023-05-31T01:37:18.367142Z","iopub.execute_input":"2023-05-31T01:37:18.372256Z","iopub.status.idle":"2023-05-31T01:37:19.801028Z","shell.execute_reply.started":"2023-05-31T01:37:18.372204Z","shell.execute_reply":"2023-05-31T01:37:19.799685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"CaHNQmo8eEyW","outputId":"d0412a1c-abc4-46c0-d74b-fd1e708289f0","execution":{"iopub.status.busy":"2023-05-31T01:37:24.899333Z","iopub.execute_input":"2023-05-31T01:37:24.899859Z","iopub.status.idle":"2023-05-31T01:37:26.303722Z","shell.execute_reply.started":"2023-05-31T01:37:24.899811Z","shell.execute_reply":"2023-05-31T01:37:26.302441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_fname = os.path.join(model_root_folder, config_dict[\"model\"] + \"_summary.txt\")\ndf.to_csv(summary_fname)","metadata":{"id":"7DVL-MCpeYPE","execution":{"iopub.status.busy":"2023-05-31T01:37:38.075554Z","iopub.execute_input":"2023-05-31T01:37:38.076291Z","iopub.status.idle":"2023-05-31T01:37:39.327679Z","shell.execute_reply.started":"2023-05-31T01:37:38.076248Z","shell.execute_reply":"2023-05-31T01:37:39.326555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.save(\"/content/\"+summary_fname, base_path = \"/content/\")","metadata":{"id":"OcQsL_i0ho50","execution":{"iopub.status.busy":"2023-05-31T01:14:05.210836Z","iopub.status.idle":"2023-05-31T01:14:05.211651Z","shell.execute_reply.started":"2023-05-31T01:14:05.211363Z","shell.execute_reply":"2023-05-31T01:14:05.211391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model weights","metadata":{"id":"IuNxVcAgjJg8"}},{"cell_type":"code","source":"model_path = os.path.join(model_root_folder, config_dict[\"model\"] + \"_model_weights.pth\")\ntorch.save(model, model_path)","metadata":{"id":"59zop-OzfTUG","execution":{"iopub.status.busy":"2023-05-31T01:14:05.213096Z","iopub.status.idle":"2023-05-31T01:14:05.213914Z","shell.execute_reply.started":"2023-05-31T01:14:05.213632Z","shell.execute_reply":"2023-05-31T01:14:05.213660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GradCAM","metadata":{"id":"9AEUqj6LYa8U"}},{"cell_type":"code","source":"from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom torchvision.models import resnet50\nfrom scipy import misc\nfrom PIL import Image\n\n\ndef reshape_transform(tensor, height=8, width=8):\n    result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n        height, width, tensor.size(2))\n\n    # Bring the channels to the first dimension,\n    # like in CNNs.\n    result = result.transpose(2, 3).transpose(1, 2)\n    return result\n\n\ndef show_grad_CAM(valid_ds, model_path, img_number):\n    model = torch.load(model_path,map_location=torch.device('cpu'))\n    if(config_dict[\"model\"] == \"alexnet\"):\n        target_layers = [model.features[10]]\n    if(config_dict[\"model\"] == \"vgg16\"):\n        print(\"yo\")\n        target_layers = [model.features[-1]]\n    if(config_dict[\"model\"] == \"vgg19\"):\n        print(\"yo\")\n        target_layers = [model.features[-1]]\n    if(config_dict[\"model\"] == \"resnet\"):\n        target_layers = [model.layer4[-1]]\n    if(config_dict[\"model\"] == \"effnetv2\"):\n        target_layers = [model.conv_head]\n    if(config_dict[\"model\"] == \"ViT\"):\n        target_layers = [model.blocks[-1].norm1]\n    if(config_dict[\"model\"] == \"DeiT\"):\n        target_layers = [model.blocks[-1].norm1]\n    img, label = valid_ds[img_number]\n    img_original = img.clone()\n    print(img.shape)\n    print(target_layers)\n    img = img.unsqueeze(0)\n    input_tensor = img # Create an input tensor image for your model..\n    # Note: input_tensor can be a batch tensor with several images!\n\n    # Construct the CAM object once, and then re-use it on many images:\n    cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform, use_cuda=False)\n\n    # You can also use it within a with statement, to make sure it is freed,\n    # In case you need to re-create it inside an outer loop:\n    # with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:\n    #   ...\n\n    # We have to specify the target we want to generate\n    # the Class Activation Maps for.\n    # If targets is None, the highest scoring category\n    # will be used for every image in the batch.\n    # Here we use ClassifierOutputTarget, but you can define your own custom targets\n    # That are, for example, combinations of categories, or specific outputs in a non standard model.\n    print(label)\n    targets = [ClassifierOutputTarget(label)]\n\n    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n    grayscale_cam = cam(input_tensor=input_tensor, targets=targets, aug_smooth = True, eigen_smooth =True)\n\n    # In this example grayscale_cam has only one image in the batch:\n    grayscale_cam = grayscale_cam[0, :]\n    plt.figure(0)\n    print(valid_ds[img_number][0].shape)\n    original_image = valid_ds[img_number][0].permute(1,2,0)\n    plt.imshow(original_image)\n    \n    img_path = os.path.join(model_root_folder, f'_Orginal_img_{img_number}_{label}.jpg')\n    #misc.imshow(original_image)\n    #temp_img = Image.fromarray(original_image.numpy(), \"RGB\")\n    #temp_img.save(\"my.png\")\n    #temp_img.show()\n    plt.savefig(img_path, bbox_inches='tight')\n\n    plt.figure(1)\n    plt.imshow(grayscale_cam)\n    gradcam_path = os.path.join(model_root_folder, f'_GradCAM_active_{img_number}_{label}.jpg')\n    plt.savefig(gradcam_path, bbox_inches='tight')\n    rgb_img = valid_ds[img_number][0].permute(1,2,0)\n    original_image = (original_image - original_image.min())/(original_image.max() - original_image.min()) \n    print(f\"grayscale_cam_max: {grayscale_cam.max()} , grayscale_cam_min: {grayscale_cam.min()}\")\n    print(f\"original_img_max: {original_image.max()} , original_img_min: {original_image.min()}\")\n    visualization = show_cam_on_image(original_image.numpy().astype(np.float32), grayscale_cam, use_rgb=True)\n    \n    plt.figure(2)\n    plt.imshow(visualization)\n    gradcam_overlay_path= os.path.join(model_root_folder, f'_GradCAM_overlap_{img_number}_{label}.jpg')\n    plt.savefig(gradcam_overlay_path, bbox_inches='tight')\n\n\n    return img_path, label, gradcam_path, gradcam_overlay_path","metadata":{"id":"7f4gC_B9j-6t","execution":{"iopub.status.busy":"2023-05-31T01:14:05.215782Z","iopub.status.idle":"2023-05-31T01:14:05.216589Z","shell.execute_reply.started":"2023-05-31T01:14:05.216303Z","shell.execute_reply":"2023-05-31T01:14:05.216331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_grad_CAM(valid_ds, model_path, img_number = 5)","metadata":{"id":"tKvkFnWcYrHf","outputId":"3c755036-f786-47d1-bcc9-2934597505dc","execution":{"iopub.status.busy":"2023-05-31T01:14:05.218061Z","iopub.status.idle":"2023-05-31T01:14:05.218881Z","shell.execute_reply.started":"2023-05-31T01:14:05.218580Z","shell.execute_reply":"2023-05-31T01:14:05.218628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature maps visualization","metadata":{"id":"x2Z_eelkv86M"}},{"cell_type":"code","source":"def intermediate_feature_maps(valid_ds, model_path, img_number):\n    model = torch.load(model_path,map_location=torch.device('cpu'))\n    # we will save the conv layer weights in this list\n    model_weights =[]\n    #we will save the 49 conv layers in this list\n    conv_layers = []# get all the model children as list\n    #model_children = list(model.children())#counter to keep count of the conv layers\n    model_children = [module for module in model.modules() if not isinstance(module, nn.Sequential)]\n    #print(model_children)\n    counter = 0#append all the conv layers and their respective wights to the list\n    for i in range(len(model_children)):\n        #print(type(model_children[i]))\n        if type(model_children[i]) == nn.Conv2d:\n            counter+=1\n            model_weights.append(model_children[i].weight)\n            conv_layers.append(model_children[i])\n        elif type(model_children[i]) == nn.Sequential:\n            for j in range(len(model_children[i])):\n                for child in model_children[i][j].children():\n                    if type(child) == nn.Conv2d:\n                        counter+=1\n                        model_weights.append(child.weight)\n                        conv_layers.append(child)\n    print(f\"Total convolution layers: {counter}\")\n    print(\"conv_layers\")\n    outputs = []\n    names = []\n    image, label = valid_ds[img_number]\n    image = image.unsqueeze(0)\n    for layer in conv_layers[0:]:\n        print(layer)\n        image = layer(image)\n        print(image.shape)\n        outputs.append(image)\n        names.append(str(layer))\n    print(len(outputs))#print feature_maps\n    for feature_map in outputs:\n        print(feature_map.shape)\n\n    processed = []\n    for feature_map in outputs:\n        feature_map = feature_map.squeeze(0)\n        gray_scale = torch.sum(feature_map,0)\n        gray_scale = gray_scale / feature_map.shape[0]\n        processed.append(gray_scale.data.cpu().numpy()) \n\n    for fm in processed:\n        print(fm.shape)\n\n    fig = plt.figure(figsize=(30, 50))\n    for i in range(min(len(processed), 40)):\n        a = fig.add_subplot(10, 4, i+1)\n        imgplot = plt.imshow(processed[i], cmap = \"gray\")\n        a.axis(\"off\")\n        a.set_title(names[i].split('(')[0], fontsize=30)\n    fmap_save_path = os.path.join(model_root_folder, f'feature_maps_{img_number}_{label}.jpg')\n    plt.savefig(fmap_save_path, bbox_inches='tight')\n\n    return fmap_save_path","metadata":{"id":"QYSCc76N0bAW","execution":{"iopub.status.busy":"2023-05-31T01:14:05.220383Z","iopub.status.idle":"2023-05-31T01:14:05.221196Z","shell.execute_reply.started":"2023-05-31T01:14:05.220926Z","shell.execute_reply":"2023-05-31T01:14:05.220955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_attention_map(img_number, get_mask=False):\n    #x = tfms_org(img)\n    #x.size()\n    model = torch.load(model_path,map_location=torch.device('cpu'))\n    img, label = valid_ds[img_number]\n    logits, att_mat = model(img.unsqueeze(0))\n\n    att_mat = torch.stack(att_mat).squeeze(1)\n\n    # Average the attention weights across all heads.\n    att_mat = torch.mean(att_mat, dim=1)\n\n    # To account for residual connections, we add an identity matrix to the\n    # attention matrix and re-normalize the weights.\n    residual_att = torch.eye(att_mat.size(1))\n    aug_att_mat = att_mat + residual_att\n    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n\n    # Recursively multiply the weight matrices\n    joint_attentions = torch.zeros(aug_att_mat.size())\n    joint_attentions[0] = aug_att_mat[0]\n\n    for n in range(1, aug_att_mat.size(0)):\n        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n\n    v = joint_attentions[-1]\n    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n    if get_mask:\n        result = cv2.resize(mask / mask.max(), img.size)\n    else:        \n        mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n        result = (mask * img).astype(\"uint8\")\n    \n    return result\n\ndef plot_attention_map(original_img, att_map):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n    ax1.set_title('Original')\n    ax2.set_title('Attention Map Last Layer')\n    _ = ax1.imshow(original_img)\n    _ = ax2.imshow(att_map)","metadata":{"id":"PGuhmTaBO-fY","execution":{"iopub.status.busy":"2023-05-31T01:14:05.222670Z","iopub.status.idle":"2023-05-31T01:14:05.223469Z","shell.execute_reply.started":"2023-05-31T01:14:05.223191Z","shell.execute_reply":"2023-05-31T01:14:05.223218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#result1 = get_attention_map(6)","metadata":{"id":"O3ZFZT1HO-d-","execution":{"iopub.status.busy":"2023-05-31T01:14:05.224996Z","iopub.status.idle":"2023-05-31T01:14:05.225796Z","shell.execute_reply.started":"2023-05-31T01:14:05.225504Z","shell.execute_reply":"2023-05-31T01:14:05.225531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(config_dict['model'] != 'ViT' or config_dict['model'] != 'DeiT'):\n    intermediate_feature_maps(valid_ds, model_path, img_number = 5)","metadata":{"id":"rL7g4mim4_xo","execution":{"iopub.status.busy":"2023-05-31T01:14:05.227258Z","iopub.status.idle":"2023-05-31T01:14:05.228070Z","shell.execute_reply.started":"2023-05-31T01:14:05.227778Z","shell.execute_reply":"2023-05-31T01:14:05.227805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb_file_save_list = glob(os.path.join(model_root_folder, \"*\"))\nfor fname in wandb_file_save_list:\n    wandb.save(fname, base_path = model_root_folder)\n","metadata":{"id":"dlaOfkrl5I8G","execution":{"iopub.status.busy":"2023-05-31T01:14:05.229510Z","iopub.status.idle":"2023-05-31T01:14:05.230316Z","shell.execute_reply.started":"2023-05-31T01:14:05.230043Z","shell.execute_reply":"2023-05-31T01:14:05.230071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WanDB Table","metadata":{"id":"K0OdkzHpVP03"}},{"cell_type":"code","source":"labels_array = [int(label) for img, label in valid_ds]","metadata":{"id":"Y3eezrsvjPgF","execution":{"iopub.status.busy":"2023-05-31T01:14:05.231761Z","iopub.status.idle":"2023-05-31T01:14:05.232557Z","shell.execute_reply.started":"2023-05-31T01:14:05.232281Z","shell.execute_reply":"2023-05-31T01:14:05.232309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_label = list(set(labels_array))\nindex_log = {}\nfor i in unique_label:\n    index_log[i] = []\n\nfor i in range(len(valid_ds)):\n    if labels_array[i]==0:\n        index_log[0] +=[i]\n    if labels_array[i]==1:\n        index_log[1] +=[i]\n    if labels_array[i]==2:\n        index_log[2] +=[i]","metadata":{"id":"ax35JzVzjf0w","execution":{"iopub.status.busy":"2023-05-31T01:14:05.234029Z","iopub.status.idle":"2023-05-31T01:14:05.234842Z","shell.execute_reply.started":"2023-05-31T01:14:05.234549Z","shell.execute_reply":"2023-05-31T01:14:05.234577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interest_index_list = []\nfor key, val in index_log.items():\n    print(key)\n    interest_index_list += val[:10]","metadata":{"id":"LZTPubfRlQq-","execution":{"iopub.status.busy":"2023-05-31T01:14:05.236307Z","iopub.status.idle":"2023-05-31T01:14:05.237126Z","shell.execute_reply.started":"2023-05-31T01:14:05.236844Z","shell.execute_reply":"2023-05-31T01:14:05.236872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in interest_index_list:\n    print(valid_ds[i][1])","metadata":{"id":"S9bspyszl_rF","execution":{"iopub.status.busy":"2023-05-31T01:14:05.238553Z","iopub.status.idle":"2023-05-31T01:14:05.239379Z","shell.execute_reply.started":"2023-05-31T01:14:05.239105Z","shell.execute_reply":"2023-05-31T01:14:05.239133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(config_dict[\"model\"] == 'ViT' or config_dict[\"model\"] == 'DeiT'):\n    table_columns = [\"Image\", \"Class label\", \"gradCAM\", \"gradCAM_overlay\", ]\n    img_table = wandb.Table(columns = table_columns)\n\n    for imgNumber in interest_index_list:\n        img, true_label, gradcam_img, gradcam_overlay = show_grad_CAM(valid_ds, model_path, img_number = imgNumber)\n        #feature_maps = intermediate_feature_maps(valid_ds, model_path, img_number = imgNumber)\n        img_table.add_data(wandb.Image(img),\\\n                        config_dict[\"target_names\"][true_label], wandb.Image(gradcam_img),\\\n                        wandb.Image(gradcam_overlay))\n\n\n    log_batch_data = wandb.Artifact(name = project_run_name, \n                                    type=\"predictions\",\n                                    description = project_name,\n                                    metadata = config_dict)\n    log_batch_data.add(img_table, \"validation_batch_record\")\n    wandb.log_artifact(log_batch_data)\nelse: \n    table_columns = [\"Image\", \"Class label\", \"gradCAM\", \"gradCAM_overlay\", \"feature_maps\"]\n    img_table = wandb.Table(columns = table_columns)\n\n    for imgNumber in interest_index_list:\n        img, true_label, gradcam_img, gradcam_overlay = show_grad_CAM(valid_ds, model_path, img_number = imgNumber)\n        feature_maps = intermediate_feature_maps(valid_ds, model_path, img_number = imgNumber)\n\n\n\n        img_table.add_data(wandb.Image(img),\\\n                        config_dict[\"target_names\"][true_label], wandb.Image(gradcam_img),\\\n                        wandb.Image(gradcam_overlay),\\\n                        wandb.Image(feature_maps))\n\n\n    log_batch_data = wandb.Artifact(name = project_run_name, \n                                    type=\"predictions\",\n                                    description = project_name,\n                                    metadata = config_dict)\n    log_batch_data.add(img_table, \"validation_batch_record\")\n    wandb.log_artifact(log_batch_data)","metadata":{"id":"98Bl9uiXPtNa","execution":{"iopub.status.busy":"2023-05-31T01:14:05.240861Z","iopub.status.idle":"2023-05-31T01:14:05.241684Z","shell.execute_reply.started":"2023-05-31T01:14:05.241386Z","shell.execute_reply":"2023-05-31T01:14:05.241414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finish WandB process","metadata":{"id":"rPBwUn5rPWul"}},{"cell_type":"code","source":"time.sleep(600)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:14:05.243135Z","iopub.status.idle":"2023-05-31T01:14:05.244410Z","shell.execute_reply.started":"2023-05-31T01:14:05.243970Z","shell.execute_reply":"2023-05-31T01:14:05.244000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"id":"GXFRlp0PK7DJ","execution":{"iopub.status.busy":"2023-05-31T01:38:07.850102Z","iopub.execute_input":"2023-05-31T01:38:07.850529Z","iopub.status.idle":"2023-05-31T01:38:15.620530Z","shell.execute_reply.started":"2023-05-31T01:38:07.850487Z","shell.execute_reply":"2023-05-31T01:38:15.619570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}